{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os, warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data set\n",
    "from sklearn.datasets import load_boston\n",
    "data=load_boston()\n",
    "X=data.data\n",
    "Y=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404,)\n",
      "(102, 13)\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "#standardise the datasetst_x=StandardScaler()\n",
    "X=st_x.fit_transform(X)\n",
    "\n",
    "#split the data into train & test\n",
    "Train_x,Test_x,Train_y,Test_y=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "print(Train_x.shape)\n",
    "print(Train_y.shape)\n",
    "print(Test_x.shape)\n",
    "print(Test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create an basic regression model and understand the Error\n",
    "lin_model=LinearRegression()\n",
    "lin_model.fit(Test_x,Test_y)\n",
    "Label_y=lin_model.predict(Test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating error of train & test set\n",
    "from sklearn import metrics\n",
    "def train_test_error(model):\n",
    "    #predicting the train set\n",
    "    Train_y_pred=model.predict(Train_x)\n",
    "    #calculate rmse\n",
    "    rmse_train=round(np.sqrt(metrics.mean_squared_error(Train_y,Train_y_pred)),3)\n",
    "    \n",
    "    #predicting the train set\n",
    "    Test_y_pred=model.predict(Test_x)\n",
    "    #calculate rmse\n",
    "    rmse_test=round(np.sqrt(metrics.mean_squared_error(Test_y,Test_y_pred)),3)\n",
    "    \n",
    "    print(\"Train_RMSE:{} Test_RMSE:{}\".format(rmse_train,rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_RMSE:5.485 Test_RMSE:4.146\n"
     ]
    }
   ],
   "source": [
    "train_test_error(lin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from the above 2 values of rmse for train & test set it is clear that there is a sign of Overfitting. \n",
    "### if a model is a good candidate for future prediction, it should have the ability to generalise any data\n",
    "### therefore, an good model should exibit the same error for train & test set\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_RMSE:4.653 Test_RMSE:4.933\n"
     ]
    }
   ],
   "source": [
    "# the most common reason behind Overfitting is due to inflation of the model coefficients. \n",
    "# To stop all or any one single coefficients from inflating, we can penalise the inflation.\n",
    "# Regularisation penalises coefficients if they inflate to a large value.\n",
    "# below we will look at 3 different types of Regularisation concepts\n",
    "# 1. Lasso - L1- (Least Shrinkage & Selection Operator)\n",
    "# 2. Ridge - L2-\n",
    "# 3. Elastic Net\n",
    "\n",
    "#Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "# Lasso Regression has 2 important Parameters\n",
    "#  1) Aplpha  - This is the regularization strength (L1). Its an multiplier term\n",
    "#  2) max_iter: No.of itterations for Gradient descent\n",
    "\n",
    "lasso_lin_model=Lasso(alpha=0.01,max_iter=100)\n",
    "lasso_lin_model.fit(Train_x,Train_y)\n",
    "\n",
    "#check variance\n",
    "train_test_error(lasso_lin_model)\n",
    "\n",
    "## The test rmse are now very close to train, Hence we have improved the genralization power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_RMSE:4.652 Test_RMSE:4.931\n"
     ]
    }
   ],
   "source": [
    "#Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "# As with Lasso Regression, Ridge also has 2 important Parameters\n",
    "#  1) Aplpha  - This is the regularization strength : Regularization improves the prediction of the coefficients\n",
    "#.              it reduces the variance of the estimates\n",
    "#               larger values straonger the regularization\n",
    "#  2) max_iter: No.of itterations for Gradient descent\n",
    "\n",
    "ridge_lin_model=Ridge(alpha=1,max_iter=100)\n",
    "ridge_lin_model.fit(Train_x,Train_y)\n",
    "\n",
    "#check variance\n",
    "train_test_error(ridge_lin_model)\n",
    "\n",
    "## The test rmse are now very close to train, Hence we have improved the genralization power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_RMSE:5.545 Test_RMSE:5.326\n"
     ]
    }
   ],
   "source": [
    "#Elastic net Regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Ealstic net regression combines both L1 & L2 regularizations to build regression model\n",
    "# Elastic net takes 2 following parameters\n",
    "# 1) Aplpha - Multiplier (L1+L2)\n",
    "# 2) l1_ratio - The elastic net mixing factor\n",
    "    # if l1_ratio = 0, implies the regularization is L2\n",
    "    # if l1_ratio =1 , implies the regularization is L1\n",
    "    # if 0<l1_ration<1, combination of L1+L2\n",
    "\n",
    "eln_lin_model=ElasticNet(alpha=1.01,l1_ratio=0.001)\n",
    "eln_lin_model.fit(Train_x,Train_y)\n",
    "\n",
    "#check variance\n",
    "train_test_error(eln_lin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above regression models,we can see that either L1 or L2 regularization has solved the Over-fitting problem i.e. variance of errors from train & test set have greatly reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
